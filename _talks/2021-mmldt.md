---
title: "Enforcing physical phenomena in system identification using Bayesian inference and stochastic models"
collection: talks
type: "Conference presentation"
permalink: /talks/2021-mmldt
venue: "Hyatt Regency Mission Bay"
date: 2021-09-29
location: "San Diego, CA, USA"
---

Conference presentation at IACM Conference on Mechanistic Machine Learning and Digital Twins for Computational Science, Engineering and Technology (MMLDT-CSET). The slides can be found [here](../files/2021-mmldt-slides.pdf).

We devise an objective function for Bayesian inference for identifying linear dynamical systems from data on partially observed nonlinear dynamical systems. We show that this objective function is robust to noisy and sparse data, and when paired with techniques to enforce physical phenomena such as stability, our method outperforms the original physics-enforcing algorithm. Our approach uses a MAP estimator that is distinct from existing estimators by accounting for both measurement noise and model error, whereas least squares-based objectives can be shown to account for exclusively one or the other. Accounting for model error means that not only will the estimator assign a low cost to models that can exactly capture the dynamics, but it will also assign a low cost to models that are nearby these exact models in the parameter space, resulting in a smoother objective function that is easier to optimize over and thus more amenable to over-parameterized problems.  Our method is therefore capable of accurately learning a basis, an initial condition, and the dynamics simultaneously, only requiring specification of the model order.  We then consider the problem of enforcing known physical phenomena in our learned model. For the problem of enforcing stability, we pair our method with a recently developed algorithm for learning stable matrices and show that our estimate outperforms methods that find the stable matrix closest to the least squares solution. Lastly, we consider Hamiltonian systems where we would like to ensure our model possesses properties such as conservation and symplecticness. To do this, we utilize the model parameterization used by Hamiltonian neural networks. We not only show that our objective function is more robust to sparse and noisy data than other methods that adopt this same parameterization, but we also show that the choice of a symplectic integrator within the objective function yields greater accuracy and certainty when compared to a non-symplectic integrator of equal order accuracy.